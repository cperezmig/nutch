<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- Put site-specific property overrides in this file. -->

<configuration>
    <property>
        <name>indexer.add.domain</name>
        <value>true</value>
        <description>Whether to add the domain field to a NutchDocument.</description>
    </property>

    <property>
        <name>db.ignore.external.links</name>
        <value>true</value>
        <description>If true, outlinks leading from a page to external hosts
        will be ignored. This will limit your crawl to the host on your seeds file.
        </description>
    </property>

    <property>
        <name>db.ignore.internal.links</name>
        <value>false</value>
        <description>If true, when adding new links to a page, links from
            the same host are ignored.  This is an effective way to limit the
            size of the link database, keeping only the highest quality
            links.
    </description>

    </property>
    <property>
        <name>db.max.outlinks.per.page</name>
        <value>-1</value>
        <description>The maximum number of outlinks that we'll process for a page.
        If this value is nonnegative (>=0), at most db.max.outlinks.per.page outlinks
        will be processed for a page; otherwise, all outlinks will be processed.
        </description>
    </property>

    <property>
        <name>db.parsemeta.to.crawldb</name>
        <value>lang,title,keywords,_rs_,_depth_</value>
        <description>Comma-separated list of parse metadata keys to transfer to the crawldb (NUTCH-779).
            Assuming for instance that the languageidentifier plugin is enabled, setting the value to 'lang'
            will copy both the key 'lang' and its value to the corresponding entry in the crawldb.
        </description>
    </property>

    <property>
        <name>parser.skip.truncated</name>
        <value>true</value>
        <description>Boolean value for whether we should skip parsing for truncated documents. By default this 
            property is activated due to extremely high levels of CPU which parsing can sometimes take.  
        </description>
    </property>

    <property>
        <name>http.content.limit</name>                                          
        <value>-1</value>
        <!--<value>65536</value>-->
        <description>The length limit for downloaded content using the http
            protocol, in bytes. If this value is nonnegative (>=0), content longer
            than it will be truncated; otherwise, no truncation at all. Do not
            confuse this setting with the file.content.limit setting.
        </description>
    </property>

    <property>
        <name>http.agent.name</name>
        <value>Mozilla/5.0</value>
    </property>
    <property>
        <name>http.agent.description</name>
        <value>compatible</value>
        <description>Further description of our bot- this text is used in
            the User-Agent header.  It appears in parenthesis after the agent name.
        </description>
    </property>

    <property>
        <name>http.agent.url</name>
        <value>+http://www.google.com/bot.html</value>
        <description>A URL to advertise in the User-Agent header.  This will 
            appear in parenthesis after the agent name. Custom dictates that this
            should be a URL of a page explaining the purpose and behavior of this
            crawler.
        </description>
    </property>

    <property>
        <name>http.agent.version</name>
        <value>Googlebot/2.1</value>
        <description>A version string to advertise in the User-Agent 
            header.</description>
    </property>

    <property>
        <name>filter.index.alternativedataflow.file</name>
        <value>alternativedataflow-indexfilter-conf.xml</value>
        <description>
            Configuration file of the alternative data flow filter. 
            Currently only supports CSV data flow.
        </description>
    </property>

    <property>
        <name>plugin.includes</name>
        <value>protocol-http|urlfilter-regex|parse-(html|tika|metatags)|index-(alternativedataflow|basic|anchor|metadata|more|links)|query-(basic|site|url)|response-(json|xml)|summary-basic|scoring-(depth|link)|indexer-solr|urlnormalizer-(pass|regex|basic)|language-identifier</value>
        <description> At the very least, I needed to add the parse-html, urlfilter-regex, and the indexer-solr.
        </description>
    </property>

    <property>
        <name>http.store.responsetime</name>
        <value>true</value>
        <description>Enables us to record the response time of the 
            host which is the time period between start connection to end 
            connection of a pages host. The response time in milliseconds
            is stored in CrawlDb in CrawlDatum's meta data under key &quot;_rs_&quot;
        </description>
    </property>

    <!-- Used only if plugin parse-metatags is enabled. -->
    <property>
        <name>metatags.names</name>
        <value>*</value>
        <description> Names of the metatags to extract, separated by ','.
            Use '*' to extract all metatags. Prefixes the names with 'metatag.'
            in the parse-metadata. For instance to index description and keywords,
            you need to activate the plugin index-metadata and set the value of the
            parameter 'index.parse.md' to 'metatag.description,metatag.keywords'.
        </description>
    </property>

    <property>
        <name>index.parse.md</name>
        <value>metatag.description,metatag.keywords</value>
        <description>
            Comma-separated list of keys to be taken from the parse metadata to generate fields.
            Can be used e.g. for 'description' or 'keywords' provided that these values are generated
            by a parser (see parse-metatags plugin)
        </description>
    </property>

    <property>
        <name>index.db.md</name>
        <value>_rs_,_depth_</value>
        <!--<value>metatag.description,metatag.keywords</value>-->
        <description>
            Comma-separated list of keys to be taken from the crawldb metadata to generate fields.
            Can be used to index values propagated from the seeds with the plugin urlmeta 
        </description>
    </property>
    
    <property>
        <name>fetcher.threads.fetch</name>
        <value>50</value>
        <description>The number of FetcherThreads the fetcher should use.
            This is also determines the maximum number of requests that are
            made at once (each FetcherThread handles one connection). The total
            number of threads running in distributed mode will be the number of
            fetcher threads * number of nodes as fetcher has one map task per node.
        </description>
    </property>

    <property>
        <name>fetcher.threads.per.queue</name>
        <value>50</value>
        <description>This number is the maximum number of threads that
            should be allowed to access a queue at one time. Setting it to 
            a value > 1 will cause the Crawl-Delay value from robots.txt to
            be ignored and the value of fetcher.server.min.delay to be used
            as a delay between successive requests to the same server instead 
            of fetcher.server.delay.
        </description>
    </property>

    <property>
        <name>fetcher.server.min.delay</name>
        <value>0.5</value>
        <description>The minimum number of seconds the fetcher will delay between 
            successive requests to the same server. This value is applicable ONLY
            if fetcher.threads.per.queue is greater than 1 (i.e. the host blocking
            is turned off).</description>
    </property>

    <property>
        <name>indexer.delete.robots.noindex</name>
        <value>true</value>
        <description>If true, robots noindex meta tag will be considered.
        </description>
    </property>

    <property>
        <name>db.update.purge.404</name>
        <value>true</value>
        <description>If true, updatedb will add purge records with status DB_GONE
            from the CrawlDB.
        </description>
    </property>

    <property>
        <name>db.url.normalizers</name>
        <value>true</value>
        <description>Normalize urls when updating crawldb</description>
    </property>

    <!-- scoring-depth properties
    Add 'scoring-depth' to the list of active plugins
    in the parameter 'plugin.includes' in order to use it.
    -->

    <property>
        <name>scoring.depth.max</name>
        <value>1000</value>
        <description>Max depth value from seed allowed by default.
            Can be overriden on a per-seed basis by specifying "_maxdepth_=VALUE"
            as a seed metadata. This plugin adds a "_depth_" metadatum to the pages
            to track the distance from the seed it was found from. 
            The depth is used to prioritise URLs in the generation step so that
            shallower pages are fetched first.
        </description>
    </property>

    <!-- linkrank scoring properties -->
    <property>
        <name>link.ignore.internal.host</name>
        <value>false</value>
        <description>Ignore outlinks to the same hostname.</description>
    </property>

    <property>
        <name>link.ignore.internal.domain</name>
        <value>false</value>
        <description>Ignore outlinks to the same domain.</description>
    </property>

    <property>
        <name>link.ignore.limit.page</name>
        <value>false</value>
        <description>Limit to only a single outlink to the same page.</description>
    </property>

    <property>
        <name>link.ignore.limit.domain</name>
        <value>false</value>
        <description>Limit to only a single outlink to the same domain.</description>
    </property> 

    <property>
        <name>indexer.score.power</name>
        <value>1</value>
        <description>Determines the power of link analyis scores.  Each
            pages's boost is set to <i>score<sup>scorePower</sup></i> where
            <i>score</i> is its link analysis score and <i>scorePower</i> is the
            value of this parameter.  This is compiled into indexes, so, when
            this is changed, pages must be re-indexed for it to take
            effect.</description>
    </property>

    <property>
        <name>fetcher.maxNum.threads</name>
        <value>100</value>  
        <description>Max number of fetch threads allowed when using fetcher.bandwidth.target. Defaults to fetcher.threads.fetch if unspecified or
            set to a value lower than it. </description>
    </property>

    <property>
        <name>db.fetch.interval.default</name>
        <value>86400</value>
        <description>The default number of seconds between re-fetches of a page (30 days).
        </description>
    </property>

    <property>
        <name>hadoop.tmp.dir</name>
        <value>/opt/hadoop_store</value>
    </property>

    <property>
        <name>index.links.outlinks.host.ignore</name>
        <value>false</value>
    </property>

    <property>
        <name>index.links.inlinks.host.ignore</name>
        <value>false</value>
    </property>

</configuration>
